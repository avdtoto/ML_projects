{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"../cluster\" style=\"font-size:20px\">All Applications (YARN)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS configuration\n",
    "\n",
    "https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n",
      "<configuration>\r\n",
      "\t<property>\r\n",
      "\t\t<name>dfs.blocksize</name>\r\n",
      "\t\t<value>16m</value>\r\n",
      "\t</property>\r\n",
      "\t<property>\r\n",
      "\t\t<name>dfs.replication</name>\r\n",
      "\t\t<value>1</value>\r\n",
      "\t</property>\r\n",
      "\t<property>\r\n",
      "\t\t<name>dfs.permissions.enabled</name>\r\n",
      "\t\t<value>false</value>\r\n",
      "\t</property>\r\n",
      "\t<property>\r\n",
      "\t\t<name>dfs.namenode.name.dir</name>\r\n",
      "\t\t<value>file:///usr/local/hadoop/hdfs/namenode</value>\r\n",
      "\t</property>\r\n",
      "\t<property>\r\n",
      "\t\t<name>dfs.datanode.data.dir</name>\r\n",
      "\t\t<value>file:///usr/local/hadoop/hdfs/datanode</value>\r\n",
      "\t</property>\r\n",
      "</configuration>\r\n"
     ]
    }
   ],
   "source": [
    "! cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datanode  namenode\r\n"
     ]
    }
   ],
   "source": [
    "! ls /usr/local/hadoop/hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\r\n",
      "\t[-appendToFile <localsrc> ... <dst>]\r\n",
      "\t[-cat [-ignoreCrc] <src> ...]\r\n",
      "\t[-checksum [-v] <src> ...]\r\n",
      "\t[-chgrp [-R] GROUP PATH...]\r\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\r\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\r\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]\r\n",
      "\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\r\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]\r\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\r\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\r\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\r\n",
      "\t[-df [-h] [<path> ...]]\r\n",
      "\t[-du [-s] [-h] [-v] [-x] <path> ...]\r\n",
      "\t[-expunge [-immediate] [-fs <path>]]\r\n",
      "\t[-find <path> ... <expression> ...]\r\n",
      "\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\r\n",
      "\t[-getfacl [-R] <path>]\r\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\r\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\r\n",
      "\t[-head <file>]\r\n",
      "\t[-help [cmd ...]]\r\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\r\n",
      "\t[-mkdir [-p] <path> ...]\r\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\r\n",
      "\t[-moveToLocal <src> <localdst>]\r\n",
      "\t[-mv <src> ... <dst>]\r\n",
      "\t[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\r\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\r\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\r\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\r\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\r\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\r\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\r\n",
      "\t[-stat [format] <path> ...]\r\n",
      "\t[-tail [-f] [-s <sleep interval>] <file>]\r\n",
      "\t[-test -[defswrz] <path>]\r\n",
      "\t[-text [-ignoreCrc] <src> ...]\r\n",
      "\t[-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ...]\r\n",
      "\t[-touchz <path> ...]\r\n",
      "\t[-truncate [-w] <length> <path> ...]\r\n",
      "\t[-usage [cmd ...]]\r\n",
      "\r\n",
      "Generic options supported are:\r\n",
      "-conf <configuration file>        specify an application configuration file\r\n",
      "-D <property=value>               define a value for a given property\r\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\r\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\r\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\r\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\r\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\r\n",
      "\r\n",
      "The general command line syntax is:\r\n",
      "command [genericOptions] [commandOptions]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy files to/from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"Test file\" > test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 384\r\n",
      "-rw-r--r-- 1 jovyan root   49768 Oct 18 15:54 bow.png\r\n",
      "-rw-r--r-- 1 jovyan root    6114 Oct 27 18:00 hdfs-basics.ipynb\r\n",
      "-rw-r--r-- 1 jovyan root  139941 Oct 18 15:54 lr.png\r\n",
      "-rw-r--r-- 1 jovyan users  25998 Oct 27 18:00 mapreduce-hw.ipynb\r\n",
      "-rw-r--r-- 1 jovyan root   11576 Oct 27 18:00 mapreduce-wordcount.ipynb\r\n",
      "-rw-r--r-- 1 jovyan root   33040 Oct 21 02:18 recsys.ipynb\r\n",
      "-rw-r--r-- 1 jovyan root   73278 Oct 18 17:14 spark-advanced.ipynb\r\n",
      "-rw-r--r-- 1 jovyan root   10381 Sep 30 00:19 spark-basics.ipynb\r\n",
      "-rw-r--r-- 1 jovyan root    8905 Sep 30 00:19 spark-sql.ipynb\r\n",
      "-rw-r--r-- 1 jovyan users     10 Oct 27 18:00 test.txt\r\n",
      "drwxr-xr-x 1 jovyan root    4096 Oct 16 00:48 wiki\r\n",
      "drwxr-xr-x 1 jovyan root    4096 Oct 19 00:36 yandex_music\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyFromLocal test.txt /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 jovyan supergroup         10 2020-10-27 18:00 /test.txt\r\n",
      "drwxrwx---   - root   supergroup          0 2020-10-27 17:59 /tmp\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyToLocal /test.txt test2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file\r\n"
     ]
    }
   ],
   "source": [
    "! cat test2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming from/to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /test.txt | wc -w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"1 2 3 4\" | hadoop fs -copyFromLocal - /test3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /test3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the replication factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t<name>dfs.replication</name>\r\n",
      "\t\t<value>1</value>\r\n"
     ]
    }
   ],
   "source": [
    "! cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml | grep -A 1 replication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -stat %r /test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication 2 set: /test.txt\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -setrep 2 /test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -stat %r /test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t<name>dfs.blocksize</name>\r\n",
      "\t\t<value>16m</value>\r\n"
     ]
    }
   ],
   "source": [
    "! cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml | grep -A 1 block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing generate.py\n"
     ]
    }
   ],
   "source": [
    "%%file generate.py\n",
    "import sys\n",
    "\n",
    "megabytes = int(sys.argv[1])\n",
    "string = \"{}M\\n\".format(megabytes)\n",
    "count = 1024 * 1024 * megabytes // len(string) - 1\n",
    "print(string * count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1M\r\n",
      "1M\r\n",
      "1M\r\n",
      "1M\r\n",
      "1M\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"./generate.py\", line 6, in <module>\r\n",
      "    print(string * count)\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! python ./generate.py 1 | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/16.txt': No such file or directory\r\n",
      "rm: `/32.txt': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm /16.txt /32.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 MB file\n",
    "! python ./generate.py 16 | hadoop fs -copyFromLocal - /16.txt\n",
    "\n",
    "# 32 MB file\n",
    "! python ./generate.py 32 | hadoop fs -copyFromLocal - /32.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://localhost:9870/fsck?ugi=jovyan&files=1&blocks=1&path=%2F\r\n",
      "FSCK started by jovyan (auth:SIMPLE) from /127.0.0.1 for path / at Tue Oct 27 18:01:09 GMT 2020\r\n",
      "\r\n",
      "/ <dir>\r\n",
      "/16.txt 16777213 bytes, replicated: replication=1, 1 block(s):  OK\r\n",
      "0. BP-417124827-172.18.0.2-1603454947911:blk_1073741827_1003 len=16777213 Live_repl=1\r\n",
      "\r\n",
      "/32.txt 33554429 bytes, replicated: replication=1, 2 block(s):  OK\r\n",
      "0. BP-417124827-172.18.0.2-1603454947911:blk_1073741828_1004 len=16777216 Live_repl=1\r\n",
      "1. BP-417124827-172.18.0.2-1603454947911:blk_1073741829_1005 len=16777213 Live_repl=1\r\n",
      "\r\n",
      "/test.txt 10 bytes, replicated: replication=2, 1 block(s):  Under replicated BP-417124827-172.18.0.2-1603454947911:blk_1073741825_1001. Target Replicas is 2 but found 1 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).\r\n",
      "0. BP-417124827-172.18.0.2-1603454947911:blk_1073741825_1001 len=10 Live_repl=1\r\n",
      "\r\n",
      "/test3.txt 8 bytes, replicated: replication=1, 1 block(s):  OK\r\n",
      "0. BP-417124827-172.18.0.2-1603454947911:blk_1073741826_1002 len=8 Live_repl=1\r\n",
      "\r\n",
      "/tmp <dir>\r\n",
      "/tmp/hadoop-yarn <dir>\r\n",
      "/tmp/hadoop-yarn/staging <dir>\r\n",
      "/tmp/hadoop-yarn/staging/history <dir>\r\n",
      "/tmp/hadoop-yarn/staging/history/done <dir>\r\n",
      "/tmp/hadoop-yarn/staging/history/done_intermediate <dir>\r\n",
      "\r\n",
      "Status: HEALTHY\r\n",
      " Number of data-nodes:\t1\r\n",
      " Number of racks:\t\t1\r\n",
      " Total dirs:\t\t\t7\r\n",
      " Total symlinks:\t\t0\r\n",
      "\r\n",
      "Replicated Blocks:\r\n",
      " Total size:\t50331660 B\r\n",
      " Total files:\t4\r\n",
      " Total blocks (validated):\t5 (avg. block size 10066332 B)\r\n",
      " Minimally replicated blocks:\t5 (100.0 %)\r\n",
      " Over-replicated blocks:\t0 (0.0 %)\r\n",
      " Under-replicated blocks:\t1 (20.0 %)\r\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\r\n",
      " Default replication factor:\t1\r\n",
      " Average block replication:\t1.0\r\n",
      " Missing blocks:\t\t0\r\n",
      " Corrupt blocks:\t\t0\r\n",
      " Missing replicas:\t\t1 (16.666666 %)\r\n",
      " Blocks queued for replication:\t0\r\n",
      "\r\n",
      "Erasure Coded Block Groups:\r\n",
      " Total size:\t0 B\r\n",
      " Total files:\t0\r\n",
      " Total block groups (validated):\t0\r\n",
      " Minimally erasure-coded block groups:\t0\r\n",
      " Over-erasure-coded block groups:\t0\r\n",
      " Under-erasure-coded block groups:\t0\r\n",
      " Unsatisfactory placement block groups:\t0\r\n",
      " Average block group size:\t0.0\r\n",
      " Missing block groups:\t\t0\r\n",
      " Corrupt block groups:\t\t0\r\n",
      " Missing internal blocks:\t0\r\n",
      " Blocks queued for replication:\t0\r\n",
      "FSCK ended at Tue Oct 27 18:01:09 GMT 2020 in 4 milliseconds\r\n",
      "\r\n",
      "\r\n",
      "The filesystem under path '/' is HEALTHY\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs fsck / -files -blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What HDFS looks like on a disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t<name>dfs.namenode.name.dir</name>\r\n",
      "\t\t<value>file:///usr/local/hadoop/hdfs/namenode</value>\r\n",
      "--\r\n",
      "\t\t<name>dfs.datanode.data.dir</name>\r\n",
      "\t\t<value>file:///usr/local/hadoop/hdfs/datanode</value>\r\n"
     ]
    }
   ],
   "source": [
    "! cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml | grep -A 1 dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/hadoop/hdfs/datanode:\r\n",
      "current  in_use.lock\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current:\r\n",
      "BP-417124827-172.18.0.2-1603454947911  VERSION\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911:\r\n",
      "current  scanner.cursor  tmp\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current:\r\n",
      "finalized  rbw\tVERSION\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized:\r\n",
      "subdir0\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0:\r\n",
      "subdir0\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0:\r\n",
      "blk_1073741825\t\t  blk_1073741827\t    blk_1073741829\r\n",
      "blk_1073741825_1001.meta  blk_1073741827_1003.meta  blk_1073741829_1005.meta\r\n",
      "blk_1073741826\t\t  blk_1073741828\r\n",
      "blk_1073741826_1002.meta  blk_1073741828_1004.meta\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/rbw:\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/tmp:\r\n"
     ]
    }
   ],
   "source": [
    "! ls -R $HADOOP_HOME/hdfs/datanode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741827 <==\r\n",
      "16M\r\n",
      "16M\r\n",
      "16M\r\n",
      "16M\r\n",
      "16M\r\n",
      "16M\r\n",
      "16M\r\n",
      "16M\r\n",
      "\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741826_1002.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000,��\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741829 <==\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741829_1005.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000�FmG�FmG�FmG�FmG�FmG�FmG�\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741828 <==\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "32M\r\n",
      "\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741825_1001.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000\u000f[H�\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741828_1004.meta <==\r\n",
      "\u0000\u0001\u0002\u0000\u0000\u0002\u0000�FmG�FmG�FmG�FmG�FmG�FmG�\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741826 <==\r\n",
      "1 2 3 4\r\n",
      "\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741825 <==\r\n",
      "Test file\r\n",
      "\r\n",
      "==> /usr/local/hadoop/hdfs/datanode/current/BP-417124827-172.18.0.2-1603454947911/current/finalized/subdir0/subdir0/blk_1073741827_1003.meta <==\r\n",
      "Ԩ!"
     ]
    }
   ],
   "source": [
    "! find $HADOOP_HOME/hdfs/datanode | grep blk | xargs head -c 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/hadoop/hdfs/namenode:\r\n",
      "current  in_use.lock\r\n",
      "\r\n",
      "/usr/local/hadoop/hdfs/namenode/current:\r\n",
      "edits_0000000000000000001-0000000000000000009  fsimage_0000000000000000000.md5\r\n",
      "edits_inprogress_0000000000000000010\t       seen_txid\r\n",
      "fsimage_0000000000000000000\t\t       VERSION\r\n"
     ]
    }
   ],
   "source": [
    "! ls -R $HADOOP_HOME/hdfs/namenode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
