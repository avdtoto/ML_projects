{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark  ML task\n",
    "\n",
    "Let's consider the classification problem into the following two classes:\n",
    "- 1 for 'US_Republican_Party_politicians'\n",
    "- 0 for 'US_Democratic_Party_politicians'\n",
    "\n",
    "Instead of a dictionary, you can use hashing.\n",
    "\n",
    "We are invited to check how the model behaves after using hashing and answer the following questions:\n",
    "1. **What roc_auc_score on the test sample is obtained when using a dictionary?**\n",
    "2. **What roc_auc_score on the test sample is obtained when switching from dictionary to hashing?**\n",
    "\n",
    "Details:\n",
    "1. Divide the samples into training and test by parity `id` articles: even for training, odd for test. Only for the training part, we count the gradients!\n",
    "2. To calculate roc_auc_score, you need to get predictions and true answers for examples from the test set. All pairs (prediction, answer) fit into memory, use it!\n",
    "3. Use `murmurhash3_32(x) % 2**14` as the hash function.\n",
    "4. Fix the random seed at the initial guess of the weights: `np.random.seed(0); weights = np.random.random(...)`\n",
    "5. Train for 100 iterations. After each iteration, call `weights_broadcast.destroy()` to remove the broadcast variable so you don't run out of memory.\n",
    "\n",
    "\n",
    "Save the solution to the `result.json` file. \n",
    "File content example:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"q1\": 0.123,\n",
    "    \"q2\": 0.456\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-12-27 23:21:07,241 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import murmurhash3_32\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# y_true - real classes\n",
    "# y_score - class 1 probabilities\n",
    "# https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "roc_auc_score(y_true=[1, 1, 0, 0], y_score=[0.8, 0.7, 0.3, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 205M\n",
      "-rw-rw-r-- 1 jovyan root  61M Mar 16  2023 categories.jsonl\n",
      "-rw-rw-r-- 1 jovyan root  387 Oct 13  2022 README.txt\n",
      "-rw-rw-r-- 1 jovyan root 144M Mar 16  2023 wiki.jsonl\n"
     ]
    }
   ],
   "source": [
    "! ls -lh wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"title\": \"April\", \"text\": \"April\\n\\nApril is the fourth month of the year, and comes between March and May. It is one of four months to have 30 days.\\n\\nApril always begins on the same day of week as July, and additionally, January in leap years. April always ends on the same day of the week as December.\\n\\nApril's flowers are the Sweet Pea and Daisy. Its birthstone is the diamond. The meaning of the diamond is innocence.\\n\\nApril comes between March and May, making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as June, September and November are later in the year.\\n\\nApril begins on the same day of the week as July every year and on the same day of the week as January in leap years. April ends on the same day of the week as December every year, as each other's last days are exactly 35 weeks (245 days) apart.\\n\\nIn common years, April starts on the same day of the week as October of the previous year, and in leap years, May of the previous year. In common years, April finishes on the same day of the week as July of the previous year, and in leap years, February and October of the previous year. In common years immediately after other common years, April starts on the same day of the week as January of the previous year, and in leap years and years immediately after that, April finishes on the same day of the week as January of the previous year.\\n\\nIn years immediately before common years, April starts on the same day of the week as September and December of the following year, and in years immediately before leap years, June of the following year. In years immediately before common years, April finishes on the same day of the week as September of the following year, and in years immediately before leap years, March and June of the following year.\\n\\nApril is a spring month in the Northern Hemisphere and an autumn/fall month in the Southern Hemisphere. In each hemisphere, it is the seasonal equivalent of October in the other.\\n\\nIt is unclear as to where April got its name. A common theory is that it comes from the Latin word \\\"aperire\\\", meaning \\\"to open\\\", referring to flowers opening in spring. Another theory is that the name could come from Aphrodite, the Greek goddess of love. It was originally the second month in the old Roman Calendar, before the start of the new year was put to January 1.\\n\\nQuite a few festivals are held in this month. In many Southeast Asian cultures, new year is celebrated in this month (including Songkran). In Western Christianity, Easter can be celebrated on a Sunday between March 22 and April 25. In Orthodox Christianity, it can fall between April 4 and May 8. At the end of the month, Central and Northern European cultures celebrate Walpurgis Night on April 30, marking the transition from winter into summer.\\n\\nPoets use \\\"April\\\" to mean the end of winter. For example: \\\"April showers bring May flowers.\\\"\\n\\n\\n\", \"url\": \"https://simple.wikipedia.org/wiki?curid=1\", \"id\": \"1\"}\n"
     ]
    }
   ],
   "source": [
    "! head -n 1 wiki/wiki.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/wiki/categories.jsonl': File exists\n",
      "copyFromLocal: `/wiki/wiki.jsonl': File exists\n",
      "copyFromLocal: `/wiki/README.txt': File exists\n",
      "copyFromLocal: `/wiki/.ipynb_checkpoints/wiki-checkpoint.jsonl': File exists\n",
      "copyFromLocal: `/wiki/.ipynb_checkpoints/categories-checkpoint.jsonl': File exists\n",
      "copyFromLocal: `/wiki/.ipynb_checkpoints/README-checkpoint.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -copyFromLocal wiki /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxr-xr-x   - jovyan supergroup          0 2023-12-27 20:08 /wiki/.ipynb_checkpoints\n",
      "-rw-r--r--   1 jovyan supergroup        387 2023-12-27 20:08 /wiki/README.txt\n",
      "-rw-r--r--   1 jovyan supergroup     60.9 M 2023-12-27 20:08 /wiki/categories.jsonl\n",
      "-rw-r--r--   1 jovyan supergroup    143.4 M 2023-12-27 20:08 /wiki/wiki.jsonl\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls -h /wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(f'[^{re.escape(string.printable)}]', ' ', text)  # replace unprintable characters with a space\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)  # and punctuation\n",
    "    words = text.lower().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def mapper(line):\n",
    "    text = json.loads(line)['text']\n",
    "    words = tokenize(text)\n",
    "    return [(word, 1) for word in set(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 839 ms, sys: 573 ms, total: 1.41 s\n",
      "Wall time: 44.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_counts = (\n",
    "    sc.textFile(\"hdfs:///wiki/wiki.jsonl\")\n",
    "    .flatMap(mapper)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_word_counts = sorted(word_counts, key=lambda x: -x[1])[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# indexes are needed for vectorization of texts\n",
    "word_to_index = {word: index for index, (word, count) in enumerate(top_word_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0), ('in', 1), ('a', 2), ('of', 3), ('is', 4)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_index.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([\"a\", \"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# second option: broadcast variable\n",
    "word_to_index_broadcast = sc.broadcast(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast variables are useful when you want to broadcast the same data to all executors:\n",
    "- dictionary in ML algorithm\n",
    "- vector of weights in ML algorithm\n",
    "\n",
    "Executors have **read-only** access to this data\n",
    "\n",
    "Send once and can be used multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    j = json.loads(line)\n",
    "    text = j['text']\n",
    "    words = tokenize(text)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for word, count in Counter(words).items():\n",
    "        if word in word_to_index_broadcast.value:\n",
    "            index = word_to_index_broadcast.value[word]\n",
    "            indices.append(index)\n",
    "            tf = count / float(len(words))\n",
    "            values.append(tf)\n",
    "    return np.array(indices), np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 23:23:02,007 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 2.0 (TID 41) (a8cd43e76584 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1563, in takeUpToNumLeft\n",
      "    except StopIteration:\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_39426/786089338.py\", line -1, in mapper\n",
      "NameError: name 'np' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2264)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2023-12-27 23:23:02,495 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 2.0 (TID 42) (a8cd43e76584 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000003/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000003/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1563, in takeUpToNumLeft\n",
      "    except StopIteration:\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000003/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_39426/786089338.py\", line -1, in mapper\n",
      "NameError: name 'np' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2264)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2023-12-27 23:23:02,837 ERROR scheduler.TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 44) (a8cd43e76584 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1563, in takeUpToNumLeft\n    except StopIteration:\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_39426/786089338.py\", line -1, in mapper\nNameError: name 'np' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1563, in takeUpToNumLeft\n    except StopIteration:\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_39426/786089338.py\", line -1, in mapper\nNameError: name 'np' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:4\u001b[0m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1568\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1565\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1567\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 1568\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1570\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   1571\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:1227\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;66;03m# Implementation note: This is implemented as a mapPartitions followed\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;66;03m# by runJob() in order to avoid having to pass a Python lambda into\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;66;03m# SparkContext#runJob.\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m-> 1227\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 44) (a8cd43e76584 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1563, in takeUpToNumLeft\n    except StopIteration:\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_39426/786089338.py\", line -1, in mapper\nNameError: name 'np' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1563, in takeUpToNumLeft\n    except StopIteration:\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1703689302506_0003/container_1703689302506_0003_01_000002/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_39426/786089338.py\", line -1, in mapper\nNameError: name 'np' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(\n",
    "    sc.textFile(\"hdfs:///wiki/wiki.jsonl\")\n",
    "    .map(mapper)\n",
    "    .take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File with article categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>April\\n\\nApril is the fourth month of the year...</td>\n",
       "      <td>April</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>August\\n\\nAugust (Aug.) is the eighth month of...</td>\n",
       "      <td>August</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               text   title  \\\n",
       "0  1  April\\n\\nApril is the fourth month of the year...   April   \n",
       "1  2  August\\n\\nAugust (Aug.) is the eighth month of...  August   \n",
       "\n",
       "                                         url  \n",
       "0  https://simple.wikipedia.org/wiki?curid=1  \n",
       "1  https://simple.wikipedia.org/wiki?curid=2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = se.read.json(\"hdfs:///wiki/wiki.jsonl\")\n",
    "wiki.registerTempTable(\"wiki\")\n",
    "wiki.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Months</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Months</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  page_id\n",
       "0   Months        1\n",
       "1   Months        2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = se.read.json(\"hdfs:///wiki/categories.jsonl\")\n",
    "categories.registerTempTable(\"categories\")\n",
    "categories.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set up logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry S. Truman\\n\\nHarry S. Truman (May 8, 188...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ronald Reagan\\n\\nRonald Wilson Reagan (; Febru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Harry S. Truman\\n\\nHarry S. Truman (May 8, 188...       0\n",
       "1  Ronald Reagan\\n\\nRonald Wilson Reagan (; Febru...       1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_train = se.sql(\"\"\"\n",
    "select\n",
    "    wiki.text,\n",
    "    cast(categories.category == 'US_Republican_Party_politicians' as int) as target\n",
    "from\n",
    "    wiki \n",
    "join\n",
    "    categories\n",
    "on wiki.id == categories.page_id\n",
    "where\n",
    "    categories.category in ('US_Republican_Party_politicians', 'US_Democratic_Party_politicians')\n",
    "    and wiki.id % 2 = 0\n",
    "\"\"\")\n",
    "joined_train.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George W. Bush\\n\\nGeorge Walker Bush (born Jul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richard Nixon\\n\\nRichard Milhous Nixon (Januar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  George W. Bush\\n\\nGeorge Walker Bush (born Jul...       1\n",
       "1  Richard Nixon\\n\\nRichard Milhous Nixon (Januar...       1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_test = se.sql(\"\"\"\n",
    "select\n",
    "    wiki.text,\n",
    "    cast(categories.category == 'US_Republican_Party_politicians' as int) as target\n",
    "from\n",
    "    wiki \n",
    "join\n",
    "    categories\n",
    "on wiki.id == categories.page_id\n",
    "where\n",
    "    categories.category in ('US_Republican_Party_politicians', 'US_Democratic_Party_politicians')\n",
    "    and wiki.id % 2 = 1\n",
    "\"\"\")\n",
    "joined_test.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mapper(row):\n",
    "    words = tokenize(row.text)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for word, count in Counter(words).items():\n",
    "        if word in word_to_index:\n",
    "            #index = word_to_index[word]\n",
    "            #index tghrough murmurhash\n",
    "            index = murmurhash3_32(word) % 2**14\n",
    "            indices.append(index)\n",
    "            tf = count / float(len(words))\n",
    "            values.append(tf)\n",
    "    return np.array(indices), np.array(values), row.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1127"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = joined_train.rdd.map(mapper)\n",
    "dataset.cache()  # cache dataset in RAM\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([13452,  2801,   567, 12213,    23,  8511, 15687, 10651,  6757,\n",
       "          5009,  8034,  1883, 15502,  4396,   430,  3998, 12180, 11662,\n",
       "         10331,  4046, 10499,  8782,  3136, 13728,  5235,  4687,  4611,\n",
       "          9489,  1475,  2588,  4753, 14727,  4338, 14668, 14660,  9441,\n",
       "          1797,  5514,   513, 14793,  2541,  2565, 15611, 10540, 12740,\n",
       "          5109, 15346, 12119,  3802,  3810,  1149,  8742, 16083,   324,\n",
       "         15072,  3140, 16351, 13616,  1097,  7702,  4846,  1008,  2172,\n",
       "          9838,   336,  8307, 13490,   361,  6965, 12053,  6600, 10674,\n",
       "         15279, 14326,  1037,  5050, 15381,  3896,  9372,  5734, 14352,\n",
       "          9185,  9724,  7209,  6712, 13946,  9603,  3573, 14660,  9045,\n",
       "          7757,    23, 12331, 12720,  2638, 11558, 13080,  5681,  3665,\n",
       "          5757,  6168,  1687,  2643,  2978, 15797, 10098, 14072,  6606,\n",
       "         11110,  5983, 13942,  2820,  1216, 10855, 10369,  3621, 12320,\n",
       "          2885,  7964,  5618,  8558,  3446, 14190,   464, 13438, 12219,\n",
       "         15196, 12994,  7159,  6302,  5760, 13652,  7753,  5481,  9786,\n",
       "          2787, 15074,  2732,  5693,  4809,  7292,  9436, 12254, 13333,\n",
       "          7845, 14679,  3236, 13202, 11414,    62,  1714, 14831, 15244,\n",
       "         10264, 14201, 11393, 16328, 12520, 10929,  2296,  1465,  4602,\n",
       "         16308, 13472,  7029,  5972,  6997,  8762,  3799,  8797, 14759,\n",
       "          5027, 10622,    97,   494, 11435,  5037, 10566,  2276,  7779,\n",
       "         11324, 14640, 11129,  6236,  7544, 10801, 16301,  6621,  9299,\n",
       "          6856,  2445, 14896, 11947, 13186, 14683, 16329,  4168,  4035,\n",
       "           843, 14377, 14219,  8353, 13107,  3073,  9202, 14720, 13626,\n",
       "          5250, 11582,  8164, 13641,  3142, 12532,  2975, 12254, 15729,\n",
       "         11438,  3767,  7548,   886, 12013, 14477,  3546, 15629,  9877,\n",
       "          9219,  5721,  8564,   745,  9259,  3974,  8780, 15944,  7582,\n",
       "         15133,  8000,   434,  9607, 13660,   972,  2069,  8326,  9451,\n",
       "         14934,  1206, 12784,  2971,  3680, 11901,  6030,  2560,  1991,\n",
       "          9307, 13615, 14816,  2399, 16375,  5833, 12999,  5904,  2110,\n",
       "          4155,   610, 14995, 12939,  1415, 12284,  4663, 12360,  6665,\n",
       "          3994,  2720,   712, 10567, 10085, 11925, 11004, 13585,  6038,\n",
       "         10599,  8716, 13998,  6222,  8866, 15742,  4546,  6477, 12728,\n",
       "         11551,  3160, 15217,  3426,  2661, 16017, 15904,  1660,   974,\n",
       "          6698, 10413, 13995,  6136, 15207,  4908,  2045, 13419,  7289,\n",
       "          1095,  4341, 14292,   233,  2651, 15641, 15947,  3707,  4710,\n",
       "          3756, 14542, 15762,  7167, 10499,  5739,  4801,  9435,  1740,\n",
       "          4849, 14176, 10088, 16006,  3068,  4803,  7117,  9223,  7884,\n",
       "          5569,  6109,  4201,  5532,  1483,  8543,  1954,  3487, 12765,\n",
       "         10296,  4088,  1892,  3526,  6875,  4073,  9128,    65,  3088,\n",
       "          8874, 10077,  9343, 10319,  3578, 14754,  4032,  2236,  1691,\n",
       "          3051, 11169, 15549, 10523, 12381,  2949, 16155, 15091,  5177,\n",
       "         15028,  1142, 14304,  8784,  7065,  3039, 15983, 15891,  1302,\n",
       "          2499, 15869,  8472, 10994,  9071,   901, 13677, 12639,   189,\n",
       "         11009, 11197,  7900,  1497,  5362, 16268, 13654, 11972, 12509,\n",
       "          5120,  8893, 12808, 13546,  9054, 15762, 12605,  9104,  7182,\n",
       "         15450,  4545,  2853, 14501, 15234,  3350,  6258,  6096, 15927,\n",
       "         15822, 12916, 10563, 11330,  2586, 11803, 12815,  4342, 14873,\n",
       "          2911,  1777, 13026, 13716,  8551,  8273, 14028,  5456, 14082,\n",
       "          2929,  9671,  5860,  1912, 12904,  6115, 13230, 14941, 12111,\n",
       "          6624, 10429, 14214,  2050,  7113,  2554, 15793, 11095,  9466,\n",
       "          8759,  4133,  7113, 12958, 12757, 11905, 15477, 13730, 12910,\n",
       "          7013, 11248, 11412, 13180,    23, 14970, 15497, 15270, 12008,\n",
       "          5583,  4748,  9492,  8403,  9582,  5847, 11797, 11877,  9257,\n",
       "          3785, 15671,   970, 10242, 11090,  8743,  7287, 12617,  5026,\n",
       "         12272, 14812, 12201,  7420, 13019,  5812,   883,  1713,  7051,\n",
       "          3118, 11234, 13884,  4608,  8962, 16197,  2467,  5954,  7193,\n",
       "          4589,  1089, 15875, 12202,  9574,   276, 11514,  9217,  6418,\n",
       "         15091,  2938, 14592, 14564, 13901,   952, 15370,  8792,  1752,\n",
       "          5828,  3891,  1085,  5588,   985,  2579,  2460,  8517,  2133,\n",
       "         15323,  6340,  6297,  5724,  5716,  1361,   165]),\n",
       "  array([0.00400534, 0.01335113, 0.03538051, 0.00133511, 0.00200267,\n",
       "         0.00133511, 0.00133511, 0.00133511, 0.00133511, 0.02403204,\n",
       "         0.07276368, 0.00066756, 0.01468625, 0.02136182, 0.00734312,\n",
       "         0.00600801, 0.00267023, 0.00200267, 0.0293725 , 0.00133511,\n",
       "         0.01869159, 0.0046729 , 0.00267023, 0.00200267, 0.00066756,\n",
       "         0.00734312, 0.00333778, 0.02136182, 0.00133511, 0.00667557,\n",
       "         0.00200267, 0.00133511, 0.00066756, 0.00133511, 0.00066756,\n",
       "         0.00066756, 0.00133511, 0.00267023, 0.00066756, 0.01201602,\n",
       "         0.00534045, 0.00200267, 0.00667557, 0.01602136, 0.0046729 ,\n",
       "         0.00066756, 0.00200267, 0.0046729 , 0.00133511, 0.00400534,\n",
       "         0.00066756, 0.00133511, 0.01869159, 0.00133511, 0.0046729 ,\n",
       "         0.00066756, 0.00267023, 0.00066756, 0.00066756, 0.00400534,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00534045, 0.00133511,\n",
       "         0.00267023, 0.01869159, 0.00066756, 0.00066756, 0.00801068,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00667557,\n",
       "         0.00133511, 0.00133511, 0.00200267, 0.00200267, 0.00066756,\n",
       "         0.00333778, 0.00400534, 0.00066756, 0.00267023, 0.00066756,\n",
       "         0.00133511, 0.00133511, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00200267, 0.00066756, 0.00934579, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00133511,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00133511, 0.00133511,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.0046729 ,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00133511, 0.00133511, 0.00066756,\n",
       "         0.00200267, 0.00200267, 0.00066756, 0.00200267, 0.00066756,\n",
       "         0.00333778, 0.00200267, 0.00066756, 0.00133511, 0.00200267,\n",
       "         0.00133511, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00200267, 0.00333778, 0.0046729 ,\n",
       "         0.00066756, 0.00066756, 0.00200267, 0.00066756, 0.00066756,\n",
       "         0.00200267, 0.00133511, 0.00066756, 0.00133511, 0.00400534,\n",
       "         0.00267023, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00133511, 0.00267023, 0.00066756, 0.00133511, 0.00133511,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00133511,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00133511, 0.00133511,\n",
       "         0.00066756, 0.00267023, 0.00333778, 0.00066756, 0.00734312,\n",
       "         0.0046729 , 0.00133511, 0.00400534, 0.00133511, 0.00133511,\n",
       "         0.00066756, 0.00267023, 0.00400534, 0.00066756, 0.00133511,\n",
       "         0.00066756, 0.00267023, 0.00200267, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00333778, 0.00066756, 0.00267023,\n",
       "         0.00200267, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00267023, 0.00400534, 0.00066756, 0.00066756,\n",
       "         0.00133511, 0.00133511, 0.00066756, 0.00066756, 0.00133511,\n",
       "         0.00133511, 0.00267023, 0.00133511, 0.00066756, 0.0046729 ,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00600801, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00200267, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00133511, 0.00066756, 0.00066756,\n",
       "         0.00133511, 0.00333778, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.0046729 , 0.0046729 , 0.00400534, 0.00066756, 0.00066756,\n",
       "         0.00400534, 0.00200267, 0.00133511, 0.00200267, 0.00066756,\n",
       "         0.00066756, 0.00400534, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00734312, 0.00066756, 0.00066756, 0.00066756, 0.00133511,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00133511, 0.00133511, 0.00066756,\n",
       "         0.00066756, 0.00133511, 0.00066756, 0.00133511, 0.00200267,\n",
       "         0.00066756, 0.00066756, 0.00200267, 0.00066756, 0.00066756,\n",
       "         0.00200267, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00400534, 0.00133511, 0.00267023, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00133511, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00267023, 0.00267023, 0.00200267, 0.00133511, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00133511, 0.00066756,\n",
       "         0.00133511, 0.00066756, 0.00066756, 0.00133511, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00267023, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00400534, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00133511, 0.00066756, 0.00133511,\n",
       "         0.00200267, 0.00066756, 0.00066756, 0.00066756, 0.00133511,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00200267, 0.00066756, 0.00133511, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00133511,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00133511, 0.00200267,\n",
       "         0.00066756, 0.00133511, 0.00066756, 0.00066756, 0.00200267,\n",
       "         0.00066756, 0.00133511, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00200267, 0.00066756, 0.00133511, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00133511,\n",
       "         0.00066756, 0.00200267, 0.00200267, 0.00133511, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00267023, 0.00066756,\n",
       "         0.00066756, 0.00133511, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00133511, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00400534, 0.00066756, 0.00066756, 0.00200267,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00133511,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00133511,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00133511, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00133511, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00133511, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00133511, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00200267,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00133511,\n",
       "         0.00066756, 0.00133511, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00200267,\n",
       "         0.00400534, 0.00066756, 0.00200267, 0.00133511, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00200267, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756, 0.00066756, 0.00066756,\n",
       "         0.00066756, 0.00066756, 0.00066756]),\n",
       "  0)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x) / (1. + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradient(weights_broadcast, loss, examples):\n",
    "    # here we accumulate the contribution to the gradient\n",
    "    gradient = np.zeros(len(weights_broadcast.value))\n",
    "    \n",
    "    for example in examples:\n",
    "        indices, values, target = example\n",
    "\n",
    "        # make a prediction with the current weights\n",
    "        p = sigmoid(values.dot(weights_broadcast.value[indices]))\n",
    "\n",
    "        # add to gradient accumulator\n",
    "        gradient[indices] += values * (p - target)\n",
    "\n",
    "        # count losses\n",
    "        p = np.clip(p, 1e-15, 1-1e-15)\n",
    "        loss.add(-(target * np.log(p) + (1 - target) * np.log(1 - p)))\n",
    "    \n",
    "    yield gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# number of examples\n",
    "N = dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.744848799507529\n",
      "epoch: 1 loss: 0.7280021693727766\n",
      "epoch: 2 loss: 0.7153134008119761\n",
      "epoch: 3 loss: 0.7057434068147871\n",
      "epoch: 4 loss: 0.6984886193851738\n",
      "epoch: 5 loss: 0.6929406099502017\n",
      "epoch: 6 loss: 0.6886453092920539\n",
      "epoch: 7 loss: 0.6852670998741577\n",
      "epoch: 8 loss: 0.6825594624226966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: 0.6803420251993822\n",
      "epoch: 10 loss: 0.6784831128513916\n",
      "epoch: 11 loss: 0.6768867106338571\n",
      "epoch: 12 loss: 0.6754828363408625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 loss: 0.6742204792992382\n",
      "epoch: 14 loss: 0.6730624440921974\n",
      "epoch: 15 loss: 0.6719815942850746\n",
      "epoch: 16 loss: 0.6709581192631996\n",
      "epoch: 17 loss: 0.6699775462611347\n",
      "epoch: 18 loss: 0.6690292942113688\n",
      "epoch: 19 loss: 0.6681056212670812\n",
      "epoch: 20 loss: 0.6672008583512752\n",
      "epoch: 21 loss: 0.6663108506010403\n",
      "epoch: 22 loss: 0.6654325500141802\n",
      "epoch: 23 loss: 0.6645637181498515\n",
      "epoch: 24 loss: 0.6637027089987201\n",
      "epoch: 25 loss: 0.6628483103011342\n",
      "epoch: 26 loss: 0.6619996275110136\n",
      "epoch: 27 loss: 0.6611559988987321\n",
      "epoch: 28 loss: 0.6603169334067054\n",
      "epoch: 29 loss: 0.6594820651403259\n",
      "epoch: 30 loss: 0.658651120028529\n",
      "epoch: 31 loss: 0.6578238913915031\n",
      "epoch: 32 loss: 0.6570002220305995\n",
      "epoch: 33 loss: 0.6561799910959705\n",
      "epoch: 34 loss: 0.6553631044552275\n",
      "epoch: 35 loss: 0.6545494876283696\n",
      "epoch: 36 loss: 0.6537390806042834\n",
      "epoch: 37 loss: 0.6529318340371119\n",
      "epoch: 38 loss: 0.652127706454767\n",
      "epoch: 39 loss: 0.6513266622099706\n",
      "epoch: 40 loss: 0.6505286699761121\n",
      "epoch: 41 loss: 0.6497337016429021\n",
      "epoch: 42 loss: 0.6489417315054293\n",
      "epoch: 43 loss: 0.6481527356685585\n",
      "epoch: 44 loss: 0.6473666916093944\n",
      "epoch: 45 loss: 0.6465835778557543\n",
      "epoch: 46 loss: 0.6458033737498085\n",
      "epoch: 47 loss: 0.6450260592742362\n",
      "epoch: 48 loss: 0.6442516149242656\n",
      "epoch: 49 loss: 0.6434800216133976\n",
      "epoch: 50 loss: 0.6427112606038565\n",
      "epoch: 51 loss: 0.6419453134551905\n",
      "epoch: 52 loss: 0.6411821619861818\n",
      "epoch: 53 loss: 0.6404217882465414\n",
      "epoch: 54 loss: 0.6396641744957735\n",
      "epoch: 55 loss: 0.6389093031873044\n",
      "epoch: 56 loss: 0.6381571569564759\n",
      "epoch: 57 loss: 0.6374077186113688\n",
      "epoch: 58 loss: 0.6366609711257085\n",
      "epoch: 59 loss: 0.6359168976333023\n",
      "epoch: 60 loss: 0.635175481423583\n",
      "epoch: 61 loss: 0.634436705937994\n",
      "epoch: 62 loss: 0.6337005547669631\n",
      "epoch: 63 loss: 0.6329670116473265\n",
      "epoch: 64 loss: 0.6322360604600865\n",
      "epoch: 65 loss: 0.6315076852283986\n",
      "epoch: 66 loss: 0.6307818701157476\n",
      "epoch: 67 loss: 0.6300585994242577\n",
      "epoch: 68 loss: 0.6293378575930918\n",
      "epoch: 69 loss: 0.6286196291969393\n",
      "epoch: 70 loss: 0.6279038989445541\n",
      "epoch: 71 loss: 0.6271906516773382\n",
      "epoch: 72 loss: 0.6264798723679629\n",
      "epoch: 73 loss: 0.6257715461190165\n",
      "epoch: 74 loss: 0.6250656581616768\n",
      "epoch: 75 loss: 0.624362193854403\n",
      "epoch: 76 loss: 0.6236611386816423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 77 loss: 0.6229624782525618\n",
      "epoch: 78 loss: 0.6222661982997812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 79 loss: 0.6215722846781265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80 loss: 0.6208807233634019\n",
      "epoch: 81 loss: 0.6201915004511555\n",
      "epoch: 82 loss: 0.6195046021554798\n",
      "epoch: 83 loss: 0.6188200148078095\n",
      "epoch: 84 loss: 0.6181377248557289\n",
      "epoch: 85 loss: 0.6174577188618019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 86 loss: 0.6167799835024038\n",
      "epoch: 87 loss: 0.6161045055665683\n",
      "epoch: 88 loss: 0.615431271954845\n",
      "epoch: 89 loss: 0.6147602696781671\n",
      "epoch: 90 loss: 0.6140914858567288\n",
      "epoch: 91 loss: 0.613424907718879\n",
      "epoch: 92 loss: 0.6127605226000195\n",
      "epoch: 93 loss: 0.6120983179415184\n",
      "epoch: 94 loss: 0.6114382812896322\n",
      "epoch: 95 loss: 0.6107804002944369\n",
      "epoch: 96 loss: 0.6101246627087763\n",
      "epoch: 97 loss: 0.6094710563872124\n",
      "epoch: 98 loss: 0.6088195692849958\n",
      "epoch: 99 loss: 0.6081701894570347\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# random weights\n",
    "weights = np.random.random(len(word_to_index))\n",
    "\n",
    "# Gradient Descent Epoch\n",
    "for i in range(100):\n",
    "    weights_broadcast = sc.broadcast(weights)\n",
    "    loss = sc.accumulator(0.0)\n",
    "    \n",
    "    # calculate the gradient\n",
    "    gradient = (\n",
    "        dataset\n",
    "        .coalesce(2)  # merge 200 cached partitions into 2\n",
    "        .mapPartitions(partial(compute_gradient, weights_broadcast, loss))\n",
    "        .reduce(lambda a, b: a + b)\n",
    "    )\n",
    "\n",
    "    # update the weights\n",
    "    weights -= 0.05 * gradient\n",
    "    \n",
    "    weights_broadcast.destroy()\n",
    "    \n",
    "    print(\"epoch:\", i, \"loss:\", loss.value / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13.311416803140952, 'gross'),\n",
       " (2.7450311746217384, 'vaucluse'),\n",
       " (2.3575921201440018, 'pole'),\n",
       " (2.3344671874054455, 'head'),\n",
       " (2.074710786844532, 'mare'),\n",
       " (1.9659612558063253, 'anymore'),\n",
       " (1.9639927949021942, 'matthews'),\n",
       " (1.7790552583003472, 'october'),\n",
       " (1.7426233344496518, 'personality'),\n",
       " (1.6467433781679168, 'subscription'),\n",
       " (1.635357246944483, 'pray'),\n",
       " (1.621877086780188, 'decade'),\n",
       " (1.5951524315786365, 'justices'),\n",
       " (1.5234701460967726, 'norrbotten')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# important words for US_Respublican_Party_politicians class\n",
    "sorted([(weights[index], word) for word, index in word_to_index.items()])[-1:-15:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-9.835122656167245, 'bayer'),\n",
       " (-4.717791084003386, 'chips'),\n",
       " (-3.2799845679931834, 'monuments'),\n",
       " (-2.8840784448482513, 'seemed'),\n",
       " (-2.4517059184940106, 'crew'),\n",
       " (-2.3763644597787272, 'vascular'),\n",
       " (-2.349988642131422, 'penn'),\n",
       " (-2.0467935144873026, 'vitamins'),\n",
       " (-1.814102358785159, 'histories'),\n",
       " (-1.698263292168816, 'nutrition'),\n",
       " (-1.5779257267630975, 'outer'),\n",
       " (-1.5732582390865713, 'conqueror'),\n",
       " (-1.5460234537813435, 'ivan'),\n",
       " (-0.9389934866307138, 'pbs'),\n",
       " (-0.9209082831959386, 'joachim')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# important words for US_Democratic_Party_politicians class\n",
    "sorted([(weights[index], word) for word, index in word_to_index.items()])[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0362651 , 0.58009835, 0.56988984, ..., 0.86778891, 0.56893277,\n",
       "       0.17996736])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(weights, examples):\n",
    "    predictions=[]\n",
    "    \n",
    "    for example in examples:\n",
    "        indices, values, target = example\n",
    "\n",
    "        # make a prediction with the current weights\n",
    "        p = sigmoid(values.dot(weights[indices]))\n",
    "        predictions.append(p)\n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1177"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = joined_test.rdd.map(mapper)\n",
    "dataset_test.cache()  # cache dataset in RAM\n",
    "dataset_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_hat = predict(weights, dataset_test.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_hat_target = [int(p > 0.5) for p in y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_target[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target = [i[2] for i in dataset_test.collect()]\n",
    "y_target[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8360652173913043"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_target, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8435521739130435"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new\n",
    "roc_auc_score(y_target, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1 = 0.8360652173913043\n",
    "q2 = 0.8435521739130435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = {\n",
    "    'q1': q1,\n",
    "    'q2': q2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"q1\": 0.8360652173913043, \"q2\": 0.8435521739130435}\n"
     ]
    }
   ],
   "source": [
    "result = json.dumps(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open(\"result.json\", \"w\")\n",
    "f.write(result)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop Spark (and YARN application)\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
