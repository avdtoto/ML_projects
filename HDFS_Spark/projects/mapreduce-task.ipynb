{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"../cluster\" style=\"font-size:20px\">All Applications (YARN)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "\n",
    "We will use the logs of listening to music artists in the Yandex.Music service.\n",
    "\n",
    "The `events.csv` file contains entries like `User,Artist,Number of plays,Number of skips`:\n",
    "```csv\n",
    "userId,artistId,plays,skips\n",
    "0,335,1,0\n",
    "0,708,1,0\n",
    "0,710,2,1\n",
    "0,815,1,1\n",
    "```\n",
    "\n",
    "We need to do the following:\n",
    "1. **Leave in the data only those users for whom the sum of plays is strictly greater than 1000. How many such users?**\n",
    "2. **In the data filtered at the first step, find the 5 most popular performers by the number of users (identifiers).**\n",
    "\n",
    "Details:\n",
    "1. Let's assume that a single user's playlist always fits in memory.\n",
    "\n",
    "Save the solution to the `result.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,artistId,plays,skips\r\n",
      "0,335,1,0\r\n",
      "0,708,1,0\r\n",
      "0,710,2,1\r\n",
      "0,815,1,1\r\n"
     ]
    }
   ],
   "source": [
    "# file content example\n",
    "! head -n 5 yandex_music/events.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   1 jovyan supergroup        254 2023-12-13 13:45 /yandex_music/README.txt\r\n",
      "-rw-r--r--   1 jovyan supergroup      3.7 M 2023-12-13 13:45 /yandex_music/artists.jsonl\r\n",
      "-rw-r--r--   1 jovyan supergroup     47.6 M 2023-12-13 13:45 /yandex_music/events.csv\r\n"
     ]
    }
   ],
   "source": [
    "# copy files to HDFS\n",
    "! hadoop fs -copyFromLocal yandex_music /\n",
    "! hadoop fs -ls -h /yandex_music\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "! head -n 10000 yandex_music/events.csv > yandex_music/evants_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3412505 yandex_music/events.csv\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l yandex_music/events.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file mapper.py\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "\n",
    "frow = True\n",
    "\n",
    "for line in sys.stdin:\n",
    "    if frow:\n",
    "        frow = False\n",
    "        continue\n",
    "    \n",
    "    element = line.strip().split(',')\n",
    "    print(element[0] + \"\\t\" + element[2])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file reducer.py\n",
    "import sys\n",
    "\n",
    "current_user = None\n",
    "total_plays = 0\n",
    "user_ids = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    user_id, plays = line.strip().split('\\t')\n",
    "\n",
    "    if current_user != user_id:\n",
    "        if current_user and total_plays > 1000:\n",
    "            print(f\"{current_user}\\t{total_plays}\")\n",
    "        current_user = user_id\n",
    "        total_plays = 0\n",
    "\n",
    "    total_plays += int(plays)\n",
    "\n",
    "if current_user and total_plays > 1000:\n",
    "    print(f\"{current_user}\\t{total_plays}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing MapReduce Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat yandex_music/evants_sample.csv | python ./mapper.py | sort -k 1,1 -t $'\\t' | python ./reducer.py > result.txt\n",
    "cat result.txt | sort -k 2,2 -t $'\\t' -n -r | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('result.txt', names = ['userId', 'values'], delimiter='\\t')\n",
    "df2 = pd.read_csv('yandex_music/evants_sample.csv')\n",
    "merged_df = pd.merge(df1, df2, on='userId', how='inner')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "# merged_df.to_csv('merged_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged_df[['userId', 'artistId']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapper2 and reduser2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%file mapper2.py\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "\n",
    "frow = True\n",
    "\n",
    "for line in sys.stdin:\n",
    "    if frow:\n",
    "        frow = False\n",
    "        continue\n",
    "    \n",
    "    element = line.strip().split(',')\n",
    "    print(element[1]+ \"\\t\" + \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%file reducer2.py\n",
    "import sys\n",
    "\n",
    "prev_key = None\n",
    "count = 0\n",
    "for line in sys.stdin:  # stream is sorted by key\n",
    "    key, value  = line.split(\"\\t\")\n",
    "    \n",
    "    if prev_key is not None and key != prev_key:\n",
    "        # new key in stream, dump previous\n",
    "        print(prev_key + \"\\t\" + str(count))\n",
    "        count = 0\n",
    "    \n",
    "    count += int(value)\n",
    "    prev_key = key\n",
    "\n",
    "# dump last key\n",
    "print(prev_key + \"\\t\" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11368\t12\n",
      "63958\t10\n",
      "59783\t10\n",
      "3629\t10\n",
      "3495\t10\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat merged.csv | python ./mapper2.py | sort -k 1,1 -t $'\\t' | python ./reducer2.py > result_singers.txt\n",
    "cat result_singers.txt | sort -k 2,2 -t $'\\t' -n -r | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('result_singers.txt', names = ['singerId', 'values'], delimiter='\\t')\n",
    "# df2 = pd.read_csv('artists.jsonl')\n",
    "# merged_df = pd.merge(df1, df2, on='userId', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting singers.py\n"
     ]
    }
   ],
   "source": [
    "%%file singers.py\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "l_ids = [11368, 3629, 259, 44148, 23524]\n",
    "for line in sys.stdin:\n",
    "    text = json.loads(line)\n",
    "    if text['artistId'] in l_ids:\n",
    "        print(text['artistName'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 10000 yandex_music/artists.jsonl > yandex_music/artist_sample.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robin Schulz\n",
      "Sia\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat yandex_music/artist_sample.jsonl | python ./singers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on a Hadoop clusterÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/output': No such file or directory\n",
      "2023-12-13 22:57:09,021 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper.py, reducer.py] [/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar] /tmp/streamjob3282643317676855417.jar tmpDir=null\n",
      "2023-12-13 22:57:10,202 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-12-13 22:57:10,515 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-12-13 22:57:11,364 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jovyan/.staging/job_1702473973502_0001\n",
      "2023-12-13 22:57:12,986 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2023-12-13 22:57:13,130 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2023-12-13 22:57:13,497 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1702473973502_0001\n",
      "2023-12-13 22:57:13,497 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-12-13 22:57:13,783 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-12-13 22:57:13,784 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-12-13 22:57:14,824 INFO impl.YarnClientImpl: Submitted application application_1702473973502_0001\n",
      "2023-12-13 22:57:14,960 INFO mapreduce.Job: The url to track the job: http://789432aeb6c1:8088/proxy/application_1702473973502_0001/\n",
      "2023-12-13 22:57:14,962 INFO mapreduce.Job: Running job: job_1702473973502_0001\n",
      "2023-12-13 22:57:48,544 INFO mapreduce.Job: Job job_1702473973502_0001 running in uber mode : false\n",
      "2023-12-13 22:57:48,554 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-12-13 22:58:35,762 INFO mapreduce.Job:  map 12% reduce 0%\n",
      "2023-12-13 22:58:37,847 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2023-12-13 22:58:41,938 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "2023-12-13 22:58:42,950 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "2023-12-13 22:58:45,161 INFO mapreduce.Job:  map 72% reduce 0%\n",
      "2023-12-13 22:58:46,175 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-12-13 22:59:00,164 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-12-13 22:59:00,185 INFO mapreduce.Job: Job job_1702473973502_0001 completed successfully\n",
      "2023-12-13 22:59:00,434 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=30125250\n",
      "\t\tFILE: Number of bytes written=61368301\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49919977\n",
      "\t\tHDFS: Number of bytes written=30537\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=84261376\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=12749824\n",
      "\t\tTotal time spent by all map tasks (ms)=164573\n",
      "\t\tTotal time spent by all reduce tasks (ms)=12451\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=164573\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=12451\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=84261376\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12749824\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3412505\n",
      "\t\tMap output records=3412502\n",
      "\t\tMap output bytes=23300240\n",
      "\t\tMap output materialized bytes=30125262\n",
      "\t\tInput split bytes=291\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4999\n",
      "\t\tReduce shuffle bytes=30125262\n",
      "\t\tReduce input records=3412502\n",
      "\t\tReduce output records=3117\n",
      "\t\tSpilled Records=6825004\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=1425\n",
      "\t\tCPU time spent (ms)=92740\n",
      "\t\tPhysical memory (bytes) snapshot=1096744960\n",
      "\t\tVirtual memory (bytes) snapshot=9112625152\n",
      "\t\tTotal committed heap usage (bytes)=937951232\n",
      "\t\tPeak Map Physical memory (bytes)=349196288\n",
      "\t\tPeak Map Virtual memory (bytes)=2198900736\n",
      "\t\tPeak Reduce Physical memory (bytes)=191844352\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2615005184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49919686\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30537\n",
      "2023-12-13 22:59:00,435 INFO streaming.StreamJob: Output directory: /output\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /output\n",
    "\n",
    "! mapred streaming \\\n",
    "  -input /yandex_music/events.csv \\\n",
    "  -output /output \\\n",
    "  -mapper \"/opt/conda/bin/python3.6 mapper.py\" \\\n",
    "  -reducer \"/opt/conda/bin/python3.6 reducer.py\" \\\n",
    "  -file mapper.py \\\n",
    "  -file reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 jovyan supergroup          0 2023-12-13 22:58 /output/_SUCCESS\r\n",
      "-rw-r--r--   1 jovyan supergroup      30537 2023-12-13 22:58 /output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4745\t6137\n",
      "989\t5443\n",
      "4688\t5022\n",
      "4689\t5003\n",
      "2051\t4847\n",
      "2102\t4784\n",
      "4627\t4682\n",
      "1016\t4657\n",
      "4575\t4646\n",
      "2266\t4641\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop fs -cat \"/output/*\" | sort -k 2,2 -t $'\\t' -n -r | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop fs -cat \"/output/*\" | sort -k 2,2 -t $'\\t' -n -r > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4745\t6137\r\n",
      "989\t5443\r\n",
      "4688\t5022\r\n",
      "4689\t5003\r\n",
      "2051\t4847\r\n",
      "2102\t4784\r\n",
      "4627\t4682\r\n",
      "1016\t4657\r\n",
      "4575\t4646\r\n",
      "2266\t4641\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 10 result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3117 result.txt\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('result.txt', names = ['userId', 'values'], delimiter='\\t')\n",
    "df2 = pd.read_csv('yandex_music/events.csv')\n",
    "merged_df = pd.merge(df1, df2, on='userId', how='inner')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "# merged_df.to_csv('merged_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged_df[['userId', 'artistId']].drop_duplicates()\n",
    "merged.to_csv('merged-2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560217 merged-2.csv\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l merged-2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = 2560217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyFromLocal merged-2.csv /yandex_music/merged-2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   1 jovyan supergroup        254 2023-12-13 13:45 /yandex_music/README.txt\n",
      "-rw-r--r--   1 jovyan supergroup      3.7 M 2023-12-13 13:45 /yandex_music/artists.jsonl\n",
      "-rw-r--r--   1 jovyan supergroup     47.6 M 2023-12-13 13:45 /yandex_music/events.csv\n",
      "-rw-r--r--   1 jovyan supergroup     25.8 M 2023-12-13 23:04 /yandex_music/merged-2.csv\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls -h /yandex_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/output-2': No such file or directory\n",
      "2023-12-13 23:05:33,979 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper2.py, reducer2.py] [/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar] /tmp/streamjob5703292509531749194.jar tmpDir=null\n",
      "2023-12-13 23:05:34,771 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-12-13 23:05:35,059 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-12-13 23:05:35,304 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jovyan/.staging/job_1702473973502_0002\n",
      "2023-12-13 23:05:35,636 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2023-12-13 23:05:35,656 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:9866\n",
      "2023-12-13 23:05:36,542 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2023-12-13 23:05:37,142 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1702473973502_0002\n",
      "2023-12-13 23:05:37,142 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-12-13 23:05:37,373 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-12-13 23:05:37,373 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-12-13 23:05:37,458 INFO impl.YarnClientImpl: Submitted application application_1702473973502_0002\n",
      "2023-12-13 23:05:37,510 INFO mapreduce.Job: The url to track the job: http://789432aeb6c1:8088/proxy/application_1702473973502_0002/\n",
      "2023-12-13 23:05:37,512 INFO mapreduce.Job: Running job: job_1702473973502_0002\n",
      "2023-12-13 23:05:45,807 INFO mapreduce.Job: Job job_1702473973502_0002 running in uber mode : false\n",
      "2023-12-13 23:05:45,809 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-12-13 23:06:04,487 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2023-12-13 23:06:05,589 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2023-12-13 23:06:06,639 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-12-13 23:06:15,761 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-12-13 23:06:16,793 INFO mapreduce.Job: Job job_1702473973502_0002 completed successfully\n",
      "2023-12-13 23:06:17,019 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=25031833\n",
      "\t\tFILE: Number of bytes written=50902036\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=27064035\n",
      "\t\tHDFS: Number of bytes written=427596\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17969152\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7971840\n",
      "\t\tTotal time spent by all map tasks (ms)=35096\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7785\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=35096\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7785\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17969152\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=7971840\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2560217\n",
      "\t\tMap output records=2560215\n",
      "\t\tMap output bytes=19911397\n",
      "\t\tMap output materialized bytes=25031839\n",
      "\t\tInput split bytes=198\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=51038\n",
      "\t\tReduce shuffle bytes=25031839\n",
      "\t\tReduce input records=2560215\n",
      "\t\tReduce output records=51038\n",
      "\t\tSpilled Records=5120430\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=654\n",
      "\t\tCPU time spent (ms)=26640\n",
      "\t\tPhysical memory (bytes) snapshot=845619200\n",
      "\t\tVirtual memory (bytes) snapshot=6946877440\n",
      "\t\tTotal committed heap usage (bytes)=704118784\n",
      "\t\tPeak Map Physical memory (bytes)=322019328\n",
      "\t\tPeak Map Virtual memory (bytes)=2165436416\n",
      "\t\tPeak Reduce Physical memory (bytes)=234250240\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2616889344\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=27063837\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=427596\n",
      "2023-12-13 23:06:17,019 INFO streaming.StreamJob: Output directory: /output-2\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /output-2\n",
    "\n",
    "! mapred streaming \\\n",
    "  -input /yandex_music/merged-2.csv \\\n",
    "  -output /output-2 \\\n",
    "  -mapper \"/opt/conda/bin/python3.6 mapper2.py\" \\\n",
    "  -reducer \"/opt/conda/bin/python3.6 reducer2.py\" \\\n",
    "  -file mapper2.py \\\n",
    "  -file reducer2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop fs -cat \"/output-2/*\" | sort -k 2,2 -t $'\\t' -n -r > result-2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11368\t2574\r\n",
      "3629\t2286\r\n",
      "259\t2208\r\n",
      "44148\t2161\r\n",
      "23524\t2110\r\n",
      "59783\t2049\r\n",
      "21042\t1925\r\n",
      "23595\t1909\r\n",
      "21643\t1902\r\n",
      "645\t1876\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 10 result-2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Guetta\n",
      "Sia\n",
      "Imagine Dragons\n",
      "ÐÐ¸-2\n",
      "Queen\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat yandex_music/artists.jsonl | python ./singers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"q1\": 3117, \"q2\": [11368, 3629, 259, 44148, 23524]}\n"
     ]
    }
   ],
   "source": [
    "res = { \"q1\": 3117, \"q2\": [11368, 3629, 259, 44148, 23524]}\n",
    "result = json.dumps(res)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"result.json\", \"w\")\n",
    "f.write(result)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
